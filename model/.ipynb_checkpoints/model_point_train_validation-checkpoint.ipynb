{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc25c04c",
   "metadata": {},
   "source": [
    " Reference: https://github.com/VidushiBhatia/U-Net-Implementation/blob/main/U_Net_for_Image_Segmentation_From_Scratch_Using_TensorFlow_v4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5abdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb850f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bulding and running deep learning model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6a888",
   "metadata": {},
   "source": [
    "### Visualize the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f311016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_black(img):\n",
    "    print('the summation of img: ', np.sum(img))\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    # h,s,v = cv2.split(hsv)\n",
    "\n",
    "    lower_val = np.array([0,0,10])\n",
    "    if np.sum(img)>40000000:\n",
    "        upper_val = np.array([256,256,200])\n",
    "    else:\n",
    "        upper_val = np.array([256,256,120])\n",
    "\n",
    "    # Threshold the HSV image to get only black colors\n",
    "    mask = cv2.inRange(hsv, lower_val, upper_val)\n",
    "\n",
    "    # Bitwise-AND mask and original image\n",
    "    res = cv2.bitwise_and(img,img, mask= mask)\n",
    "    # invert the mask to get black letters on white background\n",
    "    res2 = cv2.bitwise_not(mask)\n",
    "    return res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2a2dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMap(fileName):\n",
    "    mapName = '/home/shared/DARPA/all_patched_data/training/point/map_patches/'+fileName\n",
    "\n",
    "    legendName = '_'.join(fileName.split('_')[0:-2])+'.png'\n",
    "    labelName = '/home/shared/DARPA/all_patched_data/training/point/legend/'+legendName\n",
    "    \n",
    "    segName = '/home/shared/DARPA/all_patched_data/training/point/seg_patches/'+fileName\n",
    "    seg_converted_Name = '/home/shared/DARPA/all_patched_data/training/point/seg_patches_converted/'+fileName\n",
    "\n",
    "#     map_img = mpimg.imread(mapName)\n",
    "    map_img = cv2.imread(mapName)\n",
    "    label_img = mpimg.imread(labelName)\n",
    "    seg_img = mpimg.imread(segName)\n",
    "    seg_img_converted = mpimg.imread(seg_converted_Name)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (25,10)\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.title(\"map\")\n",
    "    plt.imshow(map_img)\n",
    "    \n",
    "    plt.subplot(1,5,2)\n",
    "    plt.title(\"filtered_map\")\n",
    "    plt.imshow(filter_black(map_img), cmap='gray')\n",
    "\n",
    "    plt.subplot(1,5,3)\n",
    "    plt.title(\"legend\")\n",
    "    plt.imshow(label_img) \n",
    "    \n",
    "    plt.subplot(1,5,4)\n",
    "    plt.title(\"segmentation\")\n",
    "    plt.spy(seg_img, markersize=5)\n",
    "\n",
    "    plt.subplot(1,5,5)\n",
    "    plt.title(\"segmentation_converted\")\n",
    "    plt.imshow(seg_img_converted)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe38345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pointName = os.listdir('/home/shared/DARPA/all_patched_data/training/point/seg_patches')\n",
    "# for i in range(70,80):\n",
    "#     plotMap(pointName[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8893d0e",
   "metadata": {},
   "source": [
    "## create a datagenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cc0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 22:57:46.403169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13867 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential([layers.RandomRotation(0.2)]) # layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "def load_train_img(filename):\n",
    "\n",
    "    mapName = '/home/shared/DARPA/all_patched_data/training/point/map_patches/'+filename[0]\n",
    "    legendName = '/home/shared/DARPA/all_patched_data/training/point/legend_converted/'+filename[1] \n",
    "\n",
    "    map_img = tf.io.read_file(mapName) # Read image file\n",
    "    map_img = tf.cast(tf.io.decode_png(map_img), dtype=tf.float32) / 255.0\n",
    "\n",
    "    legend_img = tf.io.read_file(legendName) # Read image file\n",
    "    legend_img = tf.cast(tf.io.decode_png(legend_img), dtype=tf.float32) / 255.0\n",
    "    legend_img = data_augmentation(legend_img)\n",
    "    \n",
    "    map_img = tf.concat(axis=2, values = [map_img, legend_img])\n",
    "    \n",
    "    map_img = map_img*2.0 - 1.0 # range(-1.0,1.0)\n",
    "    map_img = tf.image.resize(map_img, [256, 256])\n",
    "    \n",
    "    segName = '/home/shared/DARPA/all_patched_data/training/point/seg_patches_converted/'+filename[0]  \n",
    "    \n",
    "    legend_img = tf.io.read_file(segName) # Read image file\n",
    "    legend_img = tf.io.decode_png(legend_img)\n",
    "    legend_img = tf.cast(legend_img, dtype=tf.float32) / 255.0\n",
    "    legend_img = tf.image.resize(legend_img, [256, 256])\n",
    "    \n",
    "    return map_img, legend_img\n",
    "\n",
    "# img, seg = load_img('UT_Eureka_249211_1992_24000_geo_mosaic_1_pt_13_18.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e739b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_validation_img(filename):\n",
    "\n",
    "    mapName = '/home/shared/DARPA/all_patched_data/validation/point/map_patches/'+filename[0]\n",
    "    legendName = '/home/shared/DARPA/all_patched_data/validation/point/legend_converted/'+filename[1] \n",
    "\n",
    "    map_img = tf.io.read_file(mapName) # Read image file\n",
    "    map_img = tf.cast(tf.io.decode_png(map_img), dtype=tf.float32) / 255.0\n",
    "\n",
    "    legend_img = tf.io.read_file(legendName) # Read image file\n",
    "    legend_img = tf.cast(tf.io.decode_png(legend_img), dtype=tf.float32) / 255.0\n",
    "    \n",
    "    map_img = tf.concat(axis=2, values = [map_img, legend_img])\n",
    "    \n",
    "    map_img = map_img*2.0 - 1.0 # range(-1.0,1.0)\n",
    "    map_img = tf.image.resize(map_img, [256, 256])\n",
    "    \n",
    "    segName = '/home/shared/DARPA/all_patched_data/validation/point/seg_patches_converted/'+filename[0]  \n",
    "    \n",
    "    legend_img = tf.io.read_file(segName) # Read image file\n",
    "    legend_img = tf.io.decode_png(legend_img)\n",
    "    legend_img = tf.cast(legend_img, dtype=tf.float32) / 255.0\n",
    "    legend_img = tf.image.resize(legend_img, [256, 256])\n",
    "    \n",
    "    return map_img, legend_img\n",
    "\n",
    "# img, seg = load_img('UT_Eureka_249211_1992_24000_geo_mosaic_1_pt_13_18.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c7c2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_map_file = os.listdir('/home/shared/DARPA/all_patched_data/training/point/map_patches')\n",
    "random.shuffle(train_map_file)\n",
    "train_map_legend_names = [(x, '_'.join(x.split('_')[0:-2])+'.png') for x in train_map_file]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_map_legend_names)\n",
    "train_dataset = train_dataset.map(load_train_img)\n",
    "train_dataset = train_dataset.shuffle(6000, reshuffle_each_iteration=False).batch(120)\n",
    "\n",
    "# A peek of how BatchDataset \n",
    "# it = iter(train_dataset)\n",
    "# print(next(it))\n",
    "\n",
    "validate_map_file = os.listdir('/home/shared/DARPA/all_patched_data/validation/point/map_patches')\n",
    "validate_map_legend_names = [(x, '_'.join(x.split('_')[0:-2])+'.png') for x in validate_map_file]\n",
    "\n",
    "validate_dataset = tf.data.Dataset.from_tensor_slices(validate_map_legend_names)\n",
    "validate_dataset = validate_dataset.map(load_validation_img)\n",
    "validate_dataset = validate_dataset.batch(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24227643",
   "metadata": {},
   "source": [
    "## Constructing the U-Net Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bbcfc",
   "metadata": {},
   "source": [
    "### U-Net Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8fe5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderMiniBlock(inputs, n_filters=32, dropout_prob=0.3, max_pooling=True):\n",
    "    \"\"\"\n",
    "    This block uses multiple convolution layers, max pool, relu activation to create an architecture for learning. \n",
    "    Dropout can be added for regularization to prevent overfitting. \n",
    "    The block returns the activation values for next layer along with a skip connection which will be used in the decoder\n",
    "    \"\"\"\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,   # Kernel size   \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  3,   # Kernel size\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer='HeNormal')(conv)\n",
    "    \n",
    "    # Batch Normalization will normalize the output of the last layer based on the batch's mean and standard deviation\n",
    "    conv = BatchNormalization()(conv, training=False)\n",
    "\n",
    "    # In case of overfitting, dropout will regularize the loss and gradient computation to shrink the influence of weights on output\n",
    "    if dropout_prob > 0:     \n",
    "        conv = tf.keras.layers.Dropout(dropout_prob)(conv)\n",
    "\n",
    "    if max_pooling:\n",
    "        next_layer = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(conv)    \n",
    "    else:\n",
    "        next_layer = conv\n",
    "\n",
    "    # skip connection (without max pooling) will be input to the decoder layer to prevent information loss during transpose convolutions      \n",
    "    skip_connection = conv\n",
    "    \n",
    "    return next_layer, skip_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ba8a6",
   "metadata": {},
   "source": [
    "### U-Net Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f40fe7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecoderMiniBlock(prev_layer_input, skip_layer_input, n_filters=32):\n",
    "    \"\"\"\n",
    "    Decoder Block first uses transpose convolution to upscale the image to a bigger size and then,\n",
    "    merges the result with skip layer results from encoder block\n",
    "    Adding 2 convolutions with 'same' padding helps further increase the depth of the network for better predictions\n",
    "    The function returns the decoded layer output\n",
    "    \"\"\"\n",
    "    # Start with a transpose convolution layer to first increase the size of the image\n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,\n",
    "                 (3,3),    # Kernel size\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(prev_layer_input)\n",
    "\n",
    "    # Merge the skip connection from previous block to prevent information loss\n",
    "    merge = concatenate([up, skip_layer_input], axis=3)\n",
    "    \n",
    "    # Add 2 Conv Layers with relu activation and HeNormal initialization for further processing\n",
    "    # The parameters for the function are similar to encoder\n",
    "    conv = Conv2D(n_filters, \n",
    "                 3,     # Kernel size\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(merge)\n",
    "    conv = Conv2D(n_filters,\n",
    "                 3,   # Kernel size\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='HeNormal')(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62b6e5",
   "metadata": {},
   "source": [
    "### Compile U-Net Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "627c65cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetCompiled(input_size=(256, 256, 4), n_filters=32, n_classes=1):\n",
    "    \"\"\"\n",
    "       Combine both encoder and decoder blocks according to the U-Net research paper\n",
    "       Return the model as output \n",
    "    \"\"\"\n",
    "    # Input size represent the size of 1 image (the size used for pre-processing) \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    # Encoder includes multiple convolutional mini blocks with different maxpooling, dropout and filter parameters\n",
    "    # Observe that the filters are increasing as we go deeper into the network which will increasse the # channels of the image \n",
    "    cblock1 = EncoderMiniBlock(inputs, n_filters,dropout_prob=0, max_pooling=True)\n",
    "    cblock2 = EncoderMiniBlock(cblock1[0],n_filters*2,dropout_prob=0, max_pooling=True)\n",
    "    cblock3 = EncoderMiniBlock(cblock2[0], n_filters*4,dropout_prob=0, max_pooling=True)\n",
    "    cblock4 = EncoderMiniBlock(cblock3[0], n_filters*8,dropout_prob=0.3, max_pooling=True)\n",
    "    cblock5 = EncoderMiniBlock(cblock4[0], n_filters*16, dropout_prob=0.3, max_pooling=False) \n",
    "    \n",
    "    # Decoder includes multiple mini blocks with decreasing number of filters\n",
    "    # Observe the skip connections from the encoder are given as input to the decoder\n",
    "    # Recall the 2nd output of encoder block was skip connection, hence cblockn[1] is used\n",
    "    ublock6 = DecoderMiniBlock(cblock5[0], cblock4[1],  n_filters * 8)\n",
    "    ublock7 = DecoderMiniBlock(ublock6, cblock3[1],  n_filters * 4)\n",
    "    ublock8 = DecoderMiniBlock(ublock7, cblock2[1],  n_filters * 2)\n",
    "    ublock9 = DecoderMiniBlock(ublock8, cblock1[1],  n_filters)\n",
    "\n",
    "    # Complete the model with 1 3x3 convolution layer (Same as the prev Conv Layers)\n",
    "    # Followed by a 1x1 Conv layer to get the image to the desired size. \n",
    "    # Observe the number of channels will be equal to number of output classes\n",
    "    conv9 = Conv2D(n_filters,\n",
    "                 3,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 1, padding='same', activation=\"sigmoid\")(conv9)\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595fc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the helper function for defining the layers for the model, given the input image size\n",
    "unet = UNetCompiled(input_size=(256,256,6), n_filters=16, n_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a404769",
   "metadata": {},
   "source": [
    "### Compile and Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e6c8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "             loss= tf.keras.losses.mae, # tf.keras.losses.binary_crossentropy,  #BinaryFocalCrossentropy(gamma=2.0, from_logits=False), #\n",
    "              metrics=['accuracy', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d55f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./saved_point_model/best_filter_point_model.hdf5', \n",
    "    monitor='loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True,\n",
    "    save_freq= 100)\n",
    "#callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # When to stop? EarlyStopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc735c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 22:57:59.693243: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 899 of 6000\n",
      "2022-11-14 22:58:09.688045: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 1811 of 6000\n",
      "2022-11-14 22:58:19.691577: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 2729 of 6000\n",
      "2022-11-14 22:58:29.685828: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 3643 of 6000\n",
      "2022-11-14 22:58:39.689982: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 4561 of 6000\n",
      "2022-11-14 22:58:49.686349: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 5467 of 6000\n",
      "2022-11-14 22:58:55.464969: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2022-11-14 22:58:57.181686: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n",
      "2022-11-14 22:59:00.747625: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-11-14 22:59:00.747680: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39/209 [====>.........................] - ETA: 7:06 - loss: 0.1904 - accuracy: 5.4668e-05 - acc: 5.4668e-05"
     ]
    }
   ],
   "source": [
    "# # load weights\n",
    "# if os.path.exists(\"./saved_point_model/best_filter_point_model.hdf5\"):\n",
    "#     unet.load_weights(\"./saved_point_model/best_filter_point_model.hdf5\")\n",
    "\n",
    "# Run the model in a mini-batch fashion and compute the progress for each epoch\n",
    "results = unet.fit(train_dataset, epochs=20, callbacks=[callback1], validation_data=validate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize and save the model that you just trained \n",
    "saved_model_path = \"./saved_point_model/best_filter_point_model.h5\" \n",
    "unet.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd32cb7",
   "metadata": {},
   "source": [
    "## Evaluate Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328e6e6",
   "metadata": {},
   "source": [
    "### View Predicted Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.load_weights(\"./saved_point_model/best_filter_point_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResult(n, fileName):\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices([fileName])\n",
    "    test_dataset = test_dataset.map(load_validation_img)\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "\n",
    "    predicted = unet.predict(test_dataset)\n",
    "    \n",
    "    mapName = '/home/shared/DARPA/all_patched_data/validation/point/map_patches/'+fileName[0]\n",
    "    segName = '/home/shared/DARPA/all_patched_data/validation/point/seg_patches/'+fileName[0]\n",
    "    seg_converted_Name = '/home/shared/DARPA/all_patched_data/validation/point/seg_patches_converted/'+fileName[0]\n",
    "    legendName = '/home/shared/DARPA/all_patched_data/validation/point/legend_converted/'+fileName[1]\n",
    "\n",
    "    map_img = mpimg.imread(mapName)\n",
    "    seg_img = mpimg.imread(segName)\n",
    "    seg_converted_img = mpimg.imread(seg_converted_Name)\n",
    "\n",
    "    label_img = mpimg.imread(legendName)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (25,10)\n",
    "    \n",
    "    plt.subplot(n,5,1)\n",
    "    plt.title(\"map\")\n",
    "    plt.imshow(map_img, cmap='gray')\n",
    "\n",
    "    plt.subplot(n,5,2)\n",
    "    plt.title(\"legend_converted\")\n",
    "    plt.imshow(label_img)\n",
    "\n",
    "    plt.subplot(n,5,3)\n",
    "    plt.title(\"true segmentation\")\n",
    "    plt.spy(seg_img,markersize=2) \n",
    "    \n",
    "    plt.subplot(n,5,4)\n",
    "    plt.title(\"true converted segmentation\")\n",
    "    plt.imshow(seg_converted_img) \n",
    "    \n",
    "    plt.subplot(n,5,5)\n",
    "    plt.title(\"predicted coverted segmentation\")\n",
    "    plt.imshow(predicted[0,:,:,0])\n",
    "#     plt.spy(np.where(predicted[0,:,:,0]>0.05,1,0),  markersize=2) \n",
    "  #  print('sum of predict: ', np.sum(predicted))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "for fileName in random.sample(validate_map_legend_names, n):\n",
    "    print(fileName)\n",
    "    plotResult(n, fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834cb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-shirui_env]",
   "language": "python",
   "name": "conda-env-.conda-shirui_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
