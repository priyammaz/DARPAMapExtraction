{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b164f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from itertools import chain\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "import imageio\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from model_UNet import UNetCompiled\n",
    "from metrics import feature_f_score\n",
    "from map_mask import map_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203d028",
   "metadata": {},
   "source": [
    "### Given a map and legend name, prepare the patched data\n",
    "a subfolder \"filename_map_patches\" and \"filename_legend\" will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62d46771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_point_legend_by_template(legendPath, legendName):\n",
    "    if '1_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/1_pt.png', legendPath)\n",
    "    if '2_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/2_pt.png', legendPath)\n",
    "    if '3_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/3_pt.png', legendPath)\n",
    "    if '4_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/4_pt.png', legendPath)\n",
    "    if '5_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/5_pt.png', legendPath)\n",
    "    if 'sinkhole_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/sinkhole_pt.png', legendPath) \n",
    "    if 'joint_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/joint_pt.png', legendPath)\n",
    "    if 'horiz' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/horizontal_pt.png', legendPath)\n",
    "    if 'inclined_' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/inclined_pt.png', legendPath)\n",
    "    if 'foliation_pt' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/foliation_pt.png', legendPath)\n",
    "    if 'collapse' in legendName or 'structure' in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/collapse_pt.png', legendPath)\n",
    "    if 'bedding_pt' in legendName and 'overturn' not in legendName:\n",
    "        shutil.copy('/home/shared/DARPA/all_patched_data/point_template/bedding_pt.png', legendPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff610cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_patch(filename, legend, patch_dims = (256,256)):\n",
    "    \"\"\"\n",
    "    filename = 'VA_Lahore_bm.tif'   \n",
    "    legend = 'SOa_poly'\n",
    "    \"\"\"\n",
    "\n",
    "    filePath = os.path.join(working_dir, filename)\n",
    "    segmentation_file = filePath.split('.')[0]+'_'+legend+'.tif' \n",
    "    patch_dims = patch_dims\n",
    "    \n",
    "    map_patches_dir = os.path.join(working_dir, filename.split('.')[0]+'_map_patches')\n",
    "   \n",
    "    map_im =  cv2.imread(filePath)\n",
    "\n",
    "    patch_overlap = 32\n",
    "    patch_step = patch_dims[1]-patch_overlap\n",
    "\n",
    "    # To patchify, the (width - patch_width) mod step_size = 0\n",
    "    shift_x = (map_im.shape[0]-patch_dims[0])%patch_step\n",
    "    shift_y = (map_im.shape[1]-patch_dims[1])%patch_step\n",
    "    shift_x_left = shift_x//2\n",
    "    shift_x_right = shift_x - shift_x_left\n",
    "    shift_y_left = shift_y//2\n",
    "    shift_y_right = shift_y - shift_y_left\n",
    "\n",
    "    shift_coord =  [shift_x_left, shift_x_right, shift_y_left, shift_y_right]\n",
    "\n",
    "    map_im_cut = map_im[shift_x_left:map_im.shape[0]-shift_x_right, shift_y_left:map_im.shape[1]-shift_y_right,:]\n",
    "    map_patchs = patchify(map_im_cut, (*patch_dims,3), patch_step)\n",
    "    \n",
    "    if not os.path.exists(map_patches_dir):\n",
    "        os.mkdir(map_patches_dir)  \n",
    "        for i in range(map_patchs.shape[0]):\n",
    "            for j in range(map_patchs.shape[1]):\n",
    "                imageio.imwrite(os.path.join(map_patches_dir, '{0:02d}_{1:02d}.png'.format(i,j)), (map_patchs[i][j][0]).astype(np.uint8))\n",
    "\n",
    "\n",
    "    ## work on cropping the legend and save it to a subfolder \"filename_legend\"\n",
    "    legend_dir = os.path.join(working_dir, filename.split('.')[0]+'_legend')\n",
    "    \n",
    "    if not os.path.exists(legend_dir):\n",
    "        os.mkdir(legend_dir)\n",
    "\n",
    "    json_file = filePath.split('.')[0]+'.json'\n",
    "    with open(json_file, 'r') as f:\n",
    "        jsonData = json.load(f)\n",
    "        \n",
    "    point_coord = []\n",
    "    \n",
    "    for label_dict in jsonData['shapes']:\n",
    "        if label_dict['label'] == legend:\n",
    "            point_coord = label_dict['points']\n",
    "    if not point_coord: raise Exception(\"!!!The provided legend does not exist: \", filename, legend)\n",
    "    flatten_list = list(chain.from_iterable(point_coord))\n",
    "    \n",
    "    if point_coord[0][0] >= point_coord[1][0] or point_coord[0][1] >= point_coord[1][1]:\n",
    "        # print(\"Coordinate right is less than left:  \", filename, legend, point_coord)\n",
    "        x_low = min(int(point_coord[0][0]), int(point_coord[1][0]))\n",
    "        x_hi = max(int(point_coord[0][0]), int(point_coord[1][0]))\n",
    "        y_low = min(int(point_coord[0][1]), int(point_coord[1][1]))\n",
    "        y_hi = max(int(point_coord[0][1]), int(point_coord[1][1]))\n",
    "    elif (len(flatten_list)!=4):\n",
    "        x_coord = [x[0] for x in point_coord]\n",
    "        y_coord = [x[1] for x in point_coord]\n",
    "        x_low, y_low, x_hi, y_hi = int(min(x_coord)), int(min(y_coord)), int(max(x_coord)), int(max(y_coord))\n",
    "        # print(\"Point Coordinates number is not 4: \", filename, legend)\n",
    "    else: x_low, y_low, x_hi, y_hi = [int(x) for x in flatten_list]\n",
    "        \n",
    "    legend_coor =  [(x_low, y_low), (x_hi, y_hi)]\n",
    "    shift_pixel  = 4\n",
    "    im_crop = map_im[y_low+shift_pixel:y_hi-shift_pixel, x_low+shift_pixel:x_hi-shift_pixel] # need to resize\n",
    "\n",
    "    im_crop_resize = cv2.resize(im_crop, dsize=patch_dims, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    imageio.imwrite(os.path.join(legend_dir, legend+'.png'), (im_crop_resize).astype(np.uint8))\n",
    "    \n",
    "    # replace_point_legend_by_template(os.path.join(legend_dir, legend+'.png'), legend) \n",
    "    \n",
    "    return legend_coor, shift_coord, map_im_cut.shape, map_patchs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93474d13",
   "metadata": {},
   "source": [
    "### create a testing data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62369a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataGenerator(filename, legend, patch_dims = (256,256)):\n",
    "    \"\"\"\n",
    "    filename = 'VA_Lahore_bm.tif'\n",
    "    legend = 'Oc_poly'\n",
    "    \"\"\"\n",
    "    def load_img(patchName):\n",
    "        map_img = tf.io.read_file(patchName) # Read image file\n",
    "        map_img = tf.cast(tf.io.decode_png(map_img), dtype=tf.float32) / 255.0\n",
    "\n",
    "        # edge detection to filter out lines\n",
    "        kChannel = 1 - tf.reduce_max(map_img, axis=2) # Calculate channel K\n",
    "        binaryThresh = 40/255\n",
    "        binaryImage = tf.clip_by_value(kChannel, clip_value_min=binaryThresh, clip_value_max=1) # keep only black pixel\n",
    "        binaryImage = 2.0*(binaryImage-binaryThresh)/(1.0-binaryThresh)-1.0\n",
    "        binaryImage = tf.expand_dims(binaryImage, axis=-1)\n",
    "         \n",
    "        legendPath = os.path.join(working_dir, filename.split('.')[0]+'_legend', legend+'.png') \n",
    "\n",
    "        legend_img = tf.io.read_file(legendPath) # Read image file\n",
    "        legend_img = tf.cast(tf.io.decode_png(legend_img), dtype=tf.float32) / 255.0\n",
    "        \n",
    "        img = tf.concat(axis=2, values = [binaryImage, legend_img])\n",
    "        img = img*2.0 - 1.0 # range(-1.0,1.0)\n",
    "        resize_image = tf.image.resize(img, [256,256])\n",
    "        return resize_image\n",
    "    \n",
    "    patch_dims = patch_dims\n",
    "    patchNames = sorted(glob(os.path.join(working_dir, filename.split('.')[0]+'_map_patches/*')))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(patchNames)\n",
    "    test_dataset = test_dataset.map(load_img)   \n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddd4d2",
   "metadata": {},
   "source": [
    "### add the map_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44e6a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_mask(filename, pad_unpatch_predicted_threshold):\n",
    "    filePath = os.path.join(working_dir, filename)\n",
    "    imarray = cv2.imread(filePath)\n",
    "    gray = cv2.cvtColor(imarray, cv2.COLOR_BGR2GRAY)  # greyscale image\n",
    "    # Detect Background Color\n",
    "    pix_hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n",
    "    background_pix_value = np.argmax(pix_hist, axis=None)\n",
    "\n",
    "    # Flood fill borders\n",
    "    height, width = gray.shape[:2]\n",
    "    corners = [[0,0],[0,height-1],[width-1, 0],[width-1, height-1]]\n",
    "    for c in corners:\n",
    "        cv2.floodFill(gray, None, (c[0],c[1]), 255)\n",
    "\n",
    "    # AdaptiveThreshold to remove noise\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 4)\n",
    "\n",
    "    # Edge Detection\n",
    "    thresh_blur = cv2.GaussianBlur(thresh, (11, 11), 0)\n",
    "    canny = cv2.Canny(thresh_blur, 0, 200)\n",
    "    canny_dilate = cv2.dilate(canny, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7)))\n",
    "\n",
    "    # Finding contours for the detected edges.\n",
    "    contours, hierarchy = cv2.findContours(canny_dilate, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Keeping only the largest detected contour.\n",
    "    contour = sorted(contours, key=cv2.contourArea, reverse=True)[0]\n",
    "\n",
    "    wid, hight = pad_unpatch_predicted_threshold.shape[0], pad_unpatch_predicted_threshold.shape[1]\n",
    "    mask = np.zeros([wid, hight])\n",
    "    mask = cv2.fillPoly(mask, pts=[contour], color=(1,1,1)).astype(int)\n",
    "    masked_img = cv2.bitwise_and(pad_unpatch_predicted_threshold, mask)\n",
    "    \n",
    "    return masked_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c57af",
   "metadata": {},
   "source": [
    "### Loading a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95c50a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_dims = (256,256)\n",
    "unet = UNetCompiled(input_size=(*patch_dims,4), n_filters=16, n_classes=1)\n",
    "unet.load_weights(\"./model_point/best_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca8ccc",
   "metadata": {},
   "source": [
    "### Inference all the map in the Validation subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eceae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInferenceForEachLegend(filename, legend):\n",
    "    \"\"\"\n",
    "    filename = 'VA_Lahore_bm.tif'\n",
    "    legend = 'CZsum_poly'\n",
    "    \"\"\"\n",
    "    \n",
    "    # <basemap_name>_<feature_name>.tif\n",
    "    write_filename = filename.split('.')[0]+'_'+legend+'.tif'\n",
    "    write_filePath = os.path.join(working_dir, 'Inference', write_filename)\n",
    "\n",
    "#     if os.path.exists(write_filePath):\n",
    "#         return\n",
    "    \n",
    "    \n",
    "    legend_coor, shift_coord, map_im_cut_dims, map_patchs_dims = build_patch(filename, legend, patch_dims = (256,256))\n",
    "    \n",
    "    test_dataset = test_dataGenerator(filename = filename, legend = legend, patch_dims = (256,256))\n",
    "    \n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    predicted = unet.predict(test_dataset)\n",
    "\n",
    "    patched_predicted = np.reshape(predicted, (map_patchs_dims[0], map_patchs_dims[1], 1, 256, 256, 1))\n",
    "    \n",
    "    for i in range(patched_predicted.shape[0]):\n",
    "        for j in range(patched_predicted.shape[1]):\n",
    "            data = patched_predicted[i][j][0][:,:,0]\n",
    "            coordinates = peak_local_max(data, min_distance=25, threshold_abs=0.8, exclude_border=False)\n",
    "            final_raster = np.zeros_like(data)\n",
    "            for x, y in coordinates:\n",
    "                final_raster[x][y]= 1\n",
    "            patched_predicted[i][j][0][:,:,0] = final_raster\n",
    "    \n",
    "    unpatch_predicted = unpatchify(patched_predicted, (map_im_cut_dims[0], map_im_cut_dims[1], 1))\n",
    "    pad_unpatch_predicted = np.pad(unpatch_predicted, [(shift_coord[0], shift_coord[1]), (shift_coord[2], shift_coord[3]), (0,0)], mode='constant')\n",
    "\n",
    "    pad_unpatch_predicted = pad_unpatch_predicted.astype(int)\n",
    "    masked_img = map_mask(filename, pad_unpatch_predicted)\n",
    "\n",
    "    # expand one more dimension and repeat the pixel value in the third axis\n",
    "    final_seg = np.repeat(masked_img[:, :, np.newaxis], 3, axis=2).astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite(write_filePath, final_seg)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed5649",
   "metadata": {},
   "source": [
    "## Inference all the map in the Validation subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d93cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USCan_LakeSuperior.tif direction_of_ice_mov_pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 23:42:40.395776: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USCan_LakeSuperior.tif drumlin_pt\n",
      "NV_OutlawSprings_319708_1980_24000_geo_mosaic.tif 1_pt\n",
      "NV_OutlawSprings_319708_1980_24000_geo_mosaic.tif 2_pt\n",
      "NV_OutlawSprings_319708_1980_24000_geo_mosaic.tif 3_pt\n",
      "WA_NWSeattle.tif C14_age_pt\n",
      "WA_NWSeattle.tif IRSL_age_pt\n",
      "WA_NWSeattle.tif cone_penetrometer_te_pt\n",
      "WA_NWSeattle.tif geotechnical_boring_pt\n",
      "WA_NWSeattle.tif geotechnical_test_pi_pt\n",
      "WA_NWSeattle.tif density_probe_pt\n",
      "WA_NWSeattle.tif monitoring_or_water__pt\n",
      "WA_NWSeattle.tif field_exposure_pt\n",
      "WA_NWSeattle.tif landslide_pt\n",
      "WA_NWSeattle.tif landslide_from_winte_pt\n",
      "AZ_Inspiration_464738_1945_24000_geo_mosaic.tif 2_pt\n",
      "AZ_Inspiration_464738_1945_24000_geo_mosaic.tif 3_pt\n",
      "AZ_Inspiration_464738_1945_24000_geo_mosaic.tif 4_pt\n",
      "AZ_Inspiration_464738_1945_24000_geo_mosaic.tif 5_pt\n",
      "rectify2_LawrenceHoffmann.tif well_pt\n",
      "WY_EatonRes.tif inclined_foliation_pt\n",
      "WY_EatonRes.tif lineation_pt\n",
      "WY_EatonRes.tif vertical_foliation_pt\n",
      "WY_EatonRes.tif fault_strike_dip_pt\n",
      "pp1410b.tif Crystalline_rocks_pt\n",
      "pp1410b.tif Saprolite_pt\n",
      "pp1410b.tif Paleozoic_sedimentar_pt\n",
      "pp1410b.tif Lower_Mesozoic_redbe_pt\n",
      "pp1410b.tif Lower_Mesozoic_diaba_pt\n",
      "pp1410b.tif Jurassic_sedimentary_pt\n",
      "WA_DesMoines.tif C14_pt\n",
      "WA_DesMoines.tif paleomag_trans_pt\n",
      "WA_DesMoines.tif paleomag_reversed_pt\n",
      "WA_DesMoines.tif bedding_pt\n",
      "WA_DesMoines.tif horizontal_bedding_pt\n",
      "WA_Woodland.tif sample_locality_pt\n",
      "MT_OldBaldyMountain_265833_1989_24000_geo_mosaic.tif 1_pt\n",
      "MT_OldBaldyMountain_265833_1989_24000_geo_mosaic.tif 2_pt\n",
      "MT_OldBaldyMountain_265833_1989_24000_geo_mosaic.tif 3_pt\n",
      "MT_OldBaldyMountain_265833_1989_24000_geo_mosaic.tif 4_pt\n",
      "MT_OldBaldyMountain_265833_1989_24000_geo_mosaic.tif 5_pt\n",
      "MT_Havre.tif erratic_block_pt\n",
      "MT_Havre.tif loc_strat_section_pt\n",
      "SD_BlackHills.tif dome_pt\n",
      "SD_BlackHills.tif coll_breccia_pipe_pt\n",
      "SD_BlackHills.tif teepee_butte_pt\n",
      "SD_BlackHills.tif top_of_bed_pt\n",
      "WY_LakeOwen.tif bedding_graded_igneo_pt\n",
      "WY_LakeOwen.tif cross_bedding_igneou_pt\n",
      "WY_LakeOwen.tif channelling_igneous_pt\n",
      "WY_LakeOwen.tif bedding_overturned_pt\n",
      "WY_LakeOwen.tif bedding_inclined_pt\n",
      "WY_LakeOwen.tif bedding_vertical_pt\n",
      "WY_LakeOwen.tif mine_shaft_pt\n",
      "WY_LakeOwen.tif prospect_pit_pt\n",
      "CO_LeadvilleSouth_401083_1969_24000_geo_mosaic.tif 1_pt\n",
      "CO_LeadvilleSouth_401083_1969_24000_geo_mosaic.tif 2_pt\n",
      "CO_LeadvilleSouth_401083_1969_24000_geo_mosaic.tif 3_pt\n",
      "CO_LeadvilleSouth_401083_1969_24000_geo_mosaic.tif 4_pt\n",
      "rectify2_volkert_NJ_coast.tif well_pt\n",
      "rectify2_volkert_NJ_coast.tif station_pt\n",
      "NV_WonderMountain_320616_1972_24000_geo_mosaic.tif 2_pt\n",
      "NV_WonderMountain_320616_1972_24000_geo_mosaic.tif 3_pt\n",
      "NV_WonderMountain_320616_1972_24000_geo_mosaic.tif 4_pt\n"
     ]
    }
   ],
   "source": [
    "working_dir = '/home/shared/DARPA/eval_data_perfomer_shirui'\n",
    "tifPaths = glob(working_dir+'/*.tif')\n",
    "sorted(tifPaths)\n",
    "\n",
    "if not os.path.exists(os.path.join(working_dir, 'Inference')):\n",
    "    os.mkdir(os.path.join(working_dir, 'Inference'))\n",
    "    \n",
    "for tifPath in tifPaths:    \n",
    "    tifFile = tifPath.split('/')[-1]\n",
    "    jsonFile = tifFile.split('.')[0]+'.json'\n",
    "\n",
    "    with open(os.path.join(working_dir, jsonFile), 'r') as f:\n",
    "        jsonData = json.load(f)\n",
    "\n",
    "    for label_dir in jsonData['shapes']:\n",
    "        legend = label_dir['label']\n",
    "        if legend.endswith('_pt'):\n",
    "            print(tifFile, legend)\n",
    "            try:\n",
    "                getInferenceForEachLegend(tifFile, legend)\n",
    "            except:\n",
    "                print(\"this file with this legend has something wrong: \", tifFile, legend)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-shirui_env]",
   "language": "python",
   "name": "conda-env-.conda-shirui_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
